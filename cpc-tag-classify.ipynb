{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "\n",
    "from utils import base64_decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载并初步处理 CSV 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Cui2hee6p+eOm+S4vea4uOaIjwrpopjnm67og4zmma8K5p...</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CkErQiBQcm9ibGVtCumimOebruiDjOaZrwrlvLrng4jmjq...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CltOT0lQMjAwMiDmma7lj4rnu4RdIOi/h+ays+WNkgrpop...</td>\n",
       "      <td>[3, 82]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>CltOT0lQMjAxMSDmj5Dpq5jnu4RdIOmTuuWcsOavrwrpop...</td>\n",
       "      <td>[1, 83, 111]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>CltOT0lQMjAwMCDmj5Dpq5jnu4RdIOaWueagvOWPluaVsA...</td>\n",
       "      <td>[3, 54, 204]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5464</th>\n",
       "      <td>5465</td>\n",
       "      <td>CltUSFVQQyAyMDI0IOWInei1m10g5YuH6Zev5pyr5pel5a...</td>\n",
       "      <td>[8, 107]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5465</th>\n",
       "      <td>5466</td>\n",
       "      <td>CltUSFVQQyAyMDI0IOWInei1m10g5L2g6K+05b6X5a+577...</td>\n",
       "      <td>[2, 390]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5466</th>\n",
       "      <td>5467</td>\n",
       "      <td>CltVU0FDTzIzREVDXSBDYW5keSBDYW5lIEZlYXN0IEIK6a...</td>\n",
       "      <td>[1, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5467</th>\n",
       "      <td>5468</td>\n",
       "      <td>CltVU0FDTzIzREVDXSBDb3dudGFjdCBUcmFjaW5nIDIgQg...</td>\n",
       "      <td>[7, 45, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5468</th>\n",
       "      <td>5469</td>\n",
       "      <td>CltVU0FDTzIzREVDXSBGYXJtZXIgSm9obiBBY3R1YWxseS...</td>\n",
       "      <td>[5, 60]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5469 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text          tags\n",
       "0        1  Cui2hee6p+eOm+S4vea4uOaIjwrpopjnm67og4zmma8K5p...           [2]\n",
       "1        2  CkErQiBQcm9ibGVtCumimOebruiDjOaZrwrlvLrng4jmjq...           [1]\n",
       "2        3  CltOT0lQMjAwMiDmma7lj4rnu4RdIOi/h+ays+WNkgrpop...       [3, 82]\n",
       "3        4  CltOT0lQMjAxMSDmj5Dpq5jnu4RdIOmTuuWcsOavrwrpop...  [1, 83, 111]\n",
       "4        5  CltOT0lQMjAwMCDmj5Dpq5jnu4RdIOaWueagvOWPluaVsA...  [3, 54, 204]\n",
       "...    ...                                                ...           ...\n",
       "5464  5465  CltUSFVQQyAyMDI0IOWInei1m10g5YuH6Zev5pyr5pel5a...      [8, 107]\n",
       "5465  5466  CltUSFVQQyAyMDI0IOWInei1m10g5L2g6K+05b6X5a+577...      [2, 390]\n",
       "5466  5467  CltVU0FDTzIzREVDXSBDYW5keSBDYW5lIEZlYXN0IEIK6a...       [1, 60]\n",
       "5467  5468  CltVU0FDTzIzREVDXSBDb3dudGFjdCBUcmFjaW5nIDIgQg...   [7, 45, 60]\n",
       "5468  5469  CltVU0FDTzIzREVDXSBGYXJtZXIgSm9obiBBY3R1YWxseS...       [5, 60]\n",
       "\n",
       "[5469 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = ['id', 'text', 'tags']\n",
    "\n",
    "df = pd.read_csv('./data/algo_problems.csv', header=None, names=header)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\\n超级玛丽游戏\\n题目背景\\n本题是洛谷的试机题目，可以帮助了解洛谷的使用。\\n\\n建议完...</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>\\nA+B Problem\\n题目背景\\n强烈推荐[新用户必读帖](/discuss/sho...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>\\n[NOIP2002 普及组] 过河卒\\n题目描述\\n棋盘上 $A$ 点有一个过河卒，需要...</td>\n",
       "      <td>[3, 82]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>\\n[NOIP2011 提高组] 铺地毯\\n题目描述\\n为了准备一个独特的颁奖典礼，组织者在...</td>\n",
       "      <td>[1, 83, 111]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>\\n[NOIP2000 提高组] 方格取数\\n题目背景\\nNOIP 2000 提高组 T4\\...</td>\n",
       "      <td>[3, 54, 204]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5464</th>\n",
       "      <td>5465</td>\n",
       "      <td>\\n[THUPC 2024 初赛] 勇闯末日塔\\n题目背景\\n安宁顷刻今将逝，末日黑云伺隙来...</td>\n",
       "      <td>[8, 107]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5465</th>\n",
       "      <td>5466</td>\n",
       "      <td>\\n[THUPC 2024 初赛] 你说得对，但是 AIGC\\n题目背景\\n你说得对，但是*...</td>\n",
       "      <td>[2, 390]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5466</th>\n",
       "      <td>5467</td>\n",
       "      <td>\\n[USACO23DEC] Candy Cane Feast B\\n题目描述\\nFarme...</td>\n",
       "      <td>[1, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5467</th>\n",
       "      <td>5468</td>\n",
       "      <td>\\n[USACO23DEC] Cowntact Tracing 2 B\\n题目描述\\nFar...</td>\n",
       "      <td>[7, 45, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5468</th>\n",
       "      <td>5469</td>\n",
       "      <td>\\n[USACO23DEC] Farmer John Actually Farms B\\n题...</td>\n",
       "      <td>[5, 60]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5469 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text          tags\n",
       "0        1  \\n超级玛丽游戏\\n题目背景\\n本题是洛谷的试机题目，可以帮助了解洛谷的使用。\\n\\n建议完...           [2]\n",
       "1        2  \\nA+B Problem\\n题目背景\\n强烈推荐[新用户必读帖](/discuss/sho...           [1]\n",
       "2        3  \\n[NOIP2002 普及组] 过河卒\\n题目描述\\n棋盘上 $A$ 点有一个过河卒，需要...       [3, 82]\n",
       "3        4  \\n[NOIP2011 提高组] 铺地毯\\n题目描述\\n为了准备一个独特的颁奖典礼，组织者在...  [1, 83, 111]\n",
       "4        5  \\n[NOIP2000 提高组] 方格取数\\n题目背景\\nNOIP 2000 提高组 T4\\...  [3, 54, 204]\n",
       "...    ...                                                ...           ...\n",
       "5464  5465  \\n[THUPC 2024 初赛] 勇闯末日塔\\n题目背景\\n安宁顷刻今将逝，末日黑云伺隙来...      [8, 107]\n",
       "5465  5466  \\n[THUPC 2024 初赛] 你说得对，但是 AIGC\\n题目背景\\n你说得对，但是*...      [2, 390]\n",
       "5466  5467  \\n[USACO23DEC] Candy Cane Feast B\\n题目描述\\nFarme...       [1, 60]\n",
       "5467  5468  \\n[USACO23DEC] Cowntact Tracing 2 B\\n题目描述\\nFar...   [7, 45, 60]\n",
       "5468  5469  \\n[USACO23DEC] Farmer John Actually Farms B\\n题...       [5, 60]\n",
       "\n",
       "[5469 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base64转换\n",
    "df['text'] = df['text'].apply(base64_decode)\n",
    "\n",
    "# 标签转换\n",
    "\n",
    "df['tags'] = df['tags'].apply(lambda x: json.loads(x))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据和标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\shiquda\\AppData\\Local\\Temp\\ipykernel_24952\\4180173136.py\", line 2, in <module>\n",
      "    from transformers import BertTokenizer\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\transformers\\__init__.py\", line 26, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\transformers\\dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\transformers\\utils\\__init__.py\", line 34, in <module>\n",
      "    from .generic import (\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\transformers\\utils\\generic.py\", line 462, in <module>\n",
      "    import torch.utils._pytree as _torch_pytree\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\torch\\__init__.py\", line 2120, in <module>\n",
      "    from torch._higher_order_ops import cond\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\torch\\_higher_order_ops\\__init__.py\", line 1, in <module>\n",
      "    from .cond import cond\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\torch\\_higher_order_ops\\cond.py\", line 5, in <module>\n",
      "    import torch._subclasses.functional_tensor\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 42, in <module>\n",
      "    class FunctionalTensor(torch.Tensor):\n",
      "  File \"d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 258, in FunctionalTensor\n",
      "    cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: \n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      " (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved encodings...\n",
      "Sample input_ids: tensor([  101,   100,   100,   100,   100,   100,   100,   100,  1918,   100,\n",
      "          100,  1876,   100,   100,   100,  1951,  1916,   100,   100,   100,\n",
      "         1918,  1989,   100,   100,   100,   100,   100,   100,   100,  1951,\n",
      "         1916,   100,   100,  1636,   100,   100,   100,  1854,  1876,   100,\n",
      "         1918,   100,   100,   100,   100,   100,  1031,  1052, 18613,  2487,\n",
      "         1033,  1006,  1013,  3291,  1013,  1052, 18613,  2487,  1007,  1635,\n",
      "         1031,  1052, 18613,  2620,  1033,  1006,  1013,  3291,  1013,  1052,\n",
      "        18613,  2620,  1007,  1636,   100,  1809,   100,   100,   100,   100,\n",
      "         1031,  1862,   100,   100,   100,   100,   100,  1033,  1006,  1013,\n",
      "         6848,  1013,  2265,  1013, 22343, 21472,  2487,  1007,   100,  1918,\n",
      "          100,   100,   100,   100,   100,   100,   100,  1740,   100,   100,\n",
      "          100,   100,   100,  1916,   100,   100,  1636,   100,   100,   100,\n",
      "          100,   100,   100,  1916,   100,   100,   100,  1774,   100,   100,\n",
      "          100,   100,  1746,  1916,  1740,   100,   100,   100,  1636,  1036,\n",
      "         1036,  1036,  1008,  1008,  1008,  1008,  1008,  1008,  1008,  1008,\n",
      "         1008,  1008,  1008,  1008,  1008,  1008,  1008,  1008,  1008,  1008,\n",
      "         1008,  1008,  1001,  1001,  1001,  1001,  1012,  1012,  1012,  1012,\n",
      "         1001,  1012,  1001,  1012,  1012,  1001,  1001,  1001,  1012,  1012,\n",
      "         1012,  1012,  1012,  1001,  1001,  1012,  1012,  1012,  1012,  1001,\n",
      "         1001,  1001,  1012,  1012,  1012,  1012,  1012,  1012,  1012,  1001,\n",
      "         1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,\n",
      "         1001,  1012,  1012,  1012,  1012,  1012,  1012,  1012,  1012,  1012,\n",
      "         1012,  1012,  1001,  1012,  1012,  1012,  1001,  1001,  1012,  1012,\n",
      "         1012,  1001,  1001,  1001,  1008,  1001,  1001,  1001,  1001,  1001,\n",
      "         1001,  1001,  1001,  1012,  1001,  1012,  1001,  1001,  1012,  1001,\n",
      "         1012,  1001,  1001,  1001,  1001,  1001,  1008,  1008,  1008,  1008,\n",
      "         1008,  1008,  1008,  1001,  1001,  1001,  1001,  1001,  1001,  1001,\n",
      "         1012,  1001,  1012,  1001,  1001,  1012,  1001,  1012,  1001,  1012,\n",
      "         1012,  1012,  1001,  1008,  1008,  1008,  1012,  1008,  1008,  1008,\n",
      "         1008,  1012,  1008,  1001,  1001,  1001,  1012,  1012,  1012,  1012,\n",
      "         1001,  1012,  1012,  1012,  1001,  1001,  1012,  1012,  1012,  1001,\n",
      "         1012,  1012,  1012,  1012,  1008,  1008,  1008,  1008,  1008,  1008,\n",
      "         1008,  1008,  1008,  1008,  1001,  1001,  1012,  1012,  1012,  1012,\n",
      "         1012,  1001,  1001,  1001,  1001,  1001,  1001,  1012,  1012,  1012,\n",
      "         1012,  1008,  1008,  1008,  1008,  1008,  1008,  1008,  1008,  1008,\n",
      "         1012,  1012,  1012,  1012,  1001,  1001,  1001,  1001,  1001,  1001,\n",
      "         1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,\n",
      "         1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,\n",
      "         1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,\n",
      "         1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,\n",
      "         1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,\n",
      "         1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,\n",
      "         1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,\n",
      "         1001,  1001,  1001,  1001,  1001,  1001,  1001,  1012,  1012,  1012,\n",
      "         1001,  1012,  1012,  1012,  1012,  1012,  1012,  1001,  1012,  1001,\n",
      "         1001,  1012,  1012,  1012,  1001,  1012,  1012,  1012,  1012,  1012,\n",
      "         1012,  1001,  1012,  1001,  1001,  1012,  1012,  1012,  1001,  1012,\n",
      "         1012,  1012,  1012,  1012,  1012,  1001,  1012,  1001,  1001,  1011,\n",
      "         1011,  1011,  1011,  1011,  1011,  1011,  1011,  1011,  1011,  1011,\n",
      "         1011,  1011,  1011,  1011,  1011,  1011,  1011,  1001,  1001,  1001,\n",
      "         1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,  1001,\n",
      "         1001,   102])\n",
      "Sample labels: [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\shiquda\\AppData\\Local\\Temp\\ipykernel_24952\\845538585.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encodings = torch.load('./data/encodings.pt')\n"
     ]
    }
   ],
   "source": [
    "# 初始化 BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 将标签转换为多标签二值矩阵\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(df['tags'])\n",
    "\n",
    "# 检查文件是否存在\n",
    "if os.path.exists('./data/encodings.pt'):\n",
    "    # 如果存在，则读取文件\n",
    "    print(\"Loading saved encodings...\")\n",
    "    encodings = torch.load('./data/encodings.pt')\n",
    "else:\n",
    "    # 如果不存在，则进行分词并保存\n",
    "    print(\"Tokenizing and saving encodings...\")\n",
    "    encodings = tokenizer(df['text'].tolist(), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "    torch.save(encodings, './data/encodings.pt')\n",
    "\n",
    "# 打印 BERT 编码后的样本\n",
    "print(\"Sample input_ids:\", encodings['input_ids'][0])\n",
    "print(\"Sample labels:\", labels[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建 Dataset 和 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1031, 1920,  ..., 7393, 1006,  102],\n",
      "        [ 101,  100,  100,  ...,    0,    0,    0],\n",
      "        [ 101, 1031, 1799,  ..., 1794, 1916,  102],\n",
      "        ...,\n",
      "        [ 101,  100, 1829,  ...,    0,    0,    0],\n",
      "        [ 101, 1643, 3781,  ...,  100,  100,  102],\n",
      "        [ 101,  100,  100,  ...,  100, 1775,  102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AlgoDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# 创建 Dataset 和 DataLoader\n",
    "dataset = AlgoDataset(encodings, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# 打印 DataLoader 中的一个批次样本\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载 BERT 模型并训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\ProgramData\\anaconda3\\envs\\cpc\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=282, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 初始化BERT模型，指定多标签分类的输出单元数量\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=labels.shape[1])\n",
    "\n",
    "# 使用AdamW优化器\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "epochs_count = 3 \n",
    "\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(epochs_count):  # 为epochs添加进度条\n",
    "    total_loss = 0  # 用于累积每个epoch的损失\n",
    "    for batch in tqdm(dataloader, desc=\"Batches\", leave=True):  # 为dataloader添加进度条\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)  # 计算每个epoch的平均损失\n",
    "    print(f\"Epoch {epoch + 1}/{epochs_count} finished, Average Loss: {avg_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
